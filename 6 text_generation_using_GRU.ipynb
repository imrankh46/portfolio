{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# import library"
      ],
      "metadata": {
        "id": "a0Bml9EX4xgG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM,GRU, Dense\n"
      ],
      "metadata": {
        "id": "keiK2Zprv15d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Shakespeare dataset\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FZwZuZ6_43JB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# Download the Shakespeare dataset\n",
        "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "response = requests.get(url)\n",
        "data = response.text\n",
        "\n",
        "# Save the data to a file\n",
        "with open(\"shakespeare.txt\", \"w\") as f:\n",
        "    f.write(data)\n"
      ],
      "metadata": {
        "id": "q5Lt-phCv178"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the text from the file\n",
        "with open(\"/content/shakespeare.txt\", \"r\") as f:\n",
        "    text_data = f.read()\n",
        "\n",
        "# Split the text into words\n",
        "words = text_data.split()\n",
        "\n",
        "# Take only the first 5000 words\n",
        "words = words[:20000]\n",
        "\n",
        "# Tokenize the words\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([words])\n",
        "\n",
        "# Convert words to sequences of integers\n",
        "sequences = tokenizer.texts_to_sequences([words])[0]\n",
        "\n",
        "# Create input and output sequences for training the model\n",
        "input_sequences = []\n",
        "output_sequences = []\n",
        "for i in range(1, len(sequences)):\n",
        "    input_sequences.append(sequences[:i])\n",
        "    output_sequences.append(sequences[i])\n",
        "\n",
        "# Pad sequences to have the same length\n",
        "max_sequence_length = 50  # Set a smaller sequence length\n",
        "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre')\n",
        "output_sequences = np.array(output_sequences)\n",
        "\n",
        "# Prepare the data for training\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "X_train = input_sequences\n",
        "y_train = to_categorical(output_sequences, num_classes=vocab_size)\n",
        "\n"
      ],
      "metadata": {
        "id": "eiYInv0jv1-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train model"
      ],
      "metadata": {
        "id": "XnK3Py8C5DK1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 256, input_length=max_sequence_length))\n",
        "model.add(GRU(128))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, batch_size=8, epochs=10)"
      ],
      "metadata": {
        "id": "rDuG_ByRv2At",
        "outputId": "8e2ff7f9-388d-45b1-e171-2774bf0230e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "2500/2500 [==============================] - 37s 13ms/step - loss: 7.0900\n",
            "Epoch 2/10\n",
            "2500/2500 [==============================] - 16s 6ms/step - loss: 6.2984\n",
            "Epoch 3/10\n",
            "2500/2500 [==============================] - 16s 6ms/step - loss: 5.4687\n",
            "Epoch 4/10\n",
            "2500/2500 [==============================] - 15s 6ms/step - loss: 4.4941\n",
            "Epoch 5/10\n",
            "2500/2500 [==============================] - 16s 6ms/step - loss: 3.5071\n",
            "Epoch 6/10\n",
            "2500/2500 [==============================] - 15s 6ms/step - loss: 2.6006\n",
            "Epoch 7/10\n",
            "2500/2500 [==============================] - 15s 6ms/step - loss: 1.8307\n",
            "Epoch 8/10\n",
            "2500/2500 [==============================] - 15s 6ms/step - loss: 1.2442\n",
            "Epoch 9/10\n",
            "2500/2500 [==============================] - 16s 7ms/step - loss: 0.8300\n",
            "Epoch 10/10\n",
            "2500/2500 [==============================] - 15s 6ms/step - loss: 0.5519\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ed0a8eaa1d0>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# generate text"
      ],
      "metadata": {
        "id": "w4z2jN8n5Hna"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to generate text using the trained model\n",
        "def generate_text(seed_text, temperature=1.0, p_val=0.8, max_length=100):\n",
        "    generated_text = seed_text.lower()\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        # Tokenize the generated text\n",
        "        sequences = tokenizer.texts_to_sequences([generated_text])\n",
        "        input_sequence = pad_sequences(sequences, maxlen=max_sequence_length, padding='pre')\n",
        "\n",
        "        # Predict the next word probabilities using the model\n",
        "        next_word_probs = model.predict(input_sequence)[0]\n",
        "        next_word_probs = next_word_probs ** (1.0 / temperature)  # Apply temperature\n",
        "        next_word_probs /= np.sum(next_word_probs)  # Normalize probabilities to sum to 1\n",
        "\n",
        "        # Select the next word based on p val and probabilities\n",
        "        next_word_idx = np.random.choice(range(vocab_size), p=next_word_probs)\n",
        "        next_word = tokenizer.index_word[next_word_idx]\n",
        "\n",
        "        # Add the next word to the generated text\n",
        "        generated_text += \" \" + next_word\n",
        "\n",
        "        # If the next word is a punctuation mark or line break, end the text generation\n",
        "        if next_word in [\".\", \",\", \":\", \";\", \"!\", \"?\", \"\\n\"]:\n",
        "            break\n",
        "\n",
        "    return generated_text\n",
        "# Generate text with temperature and p val control\n",
        "seed_text = \"he fall in love\"\n",
        "generated_text = generate_text(seed_text, temperature=0.3, p_val=0.9, max_length=30)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "EaqHd5sk4BnR",
        "outputId": "d0212442-9c34-4552-b2d0-802589755d2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "he fall in love him or so, dishonour'd that he that end: like to find you any thing, cominius: with the other lose, never was never be the motive of our so frank donation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generated_text, end=\"\")"
      ],
      "metadata": {
        "id": "Dkq6-Qvxv2Eh",
        "outputId": "b7feec02-86a8-48f6-f0c9-2fff8e51e96b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "he fall in love him or so, dishonour'd that he that end: like to find you any thing, cominius: with the other lose, never was never be the motive of our so frank donation."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HQsV9tNcv2Gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "86iqdgGov2IU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}